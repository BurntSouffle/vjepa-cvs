# Experiment 1: SAGES + Endoscapes with Current Config (Baseline)
# ================================================================
# Purpose: Test if more data (SAGES + Endoscapes) helps
# Target: RunPod
#
# Configuration:
#   - Dataset: SAGES + Endoscapes combined
#   - Model: ViT-L frozen
#   - Pooling: Mean (current)
#   - Head: MLP (current)

# Paths
data:
  sages_root: "/workspace/sages_cvs_challenge_2025_r1"
  endoscapes_root: "/workspace/endoscapes"
  results_dir: "results/exp1_sages_baseline"
  dataset_type: "combined"  # "combined", "sages", or "endoscapes"

# Model
model:
  name: "facebook/vjepa2-vitl-fpc16-256-ssv2"
  freeze_backbone: true
  hidden_dim: 1024
  classifier_hidden: 512
  num_classes: 3
  dropout: 0.5
  # Experiment-specific settings
  pooling_type: "mean"      # "mean" or "attention"
  head_type: "mlp"          # "mlp" or "simple"
  attention_heads: 8
  attention_dropout: 0.1

# Data loading
dataset:
  num_frames: 16
  frame_step: 25  # Endoscapes frame step
  resolution: 256
  augment_train: true
  horizontal_flip_prob: 0.5
  sages_val_ratio: 0.2

# Training
training:
  batch_size: 32
  num_workers: 4
  epochs: 50
  optimizer: "adamw"
  learning_rate: 5.0e-4
  weight_decay: 0.1
  scheduler: "cosine"
  warmup_epochs: 5
  min_lr: 1.0e-6
  early_stopping: true
  patience: 10
  grad_clip: 1.0

# Loss
loss:
  type: "bce_with_logits"
  use_class_weights: false
  pos_weight: [1.0, 1.0, 1.0]

# Evaluation
evaluation:
  metrics: ["mAP", "AP_per_class", "balanced_accuracy", "f1"]
  threshold: 0.5

# Logging
logging:
  log_every_n_steps: 10
  save_every_n_epochs: 5

# Reproducibility
seed: 42

# Experiment metadata
experiment:
  name: "exp1_sages_baseline"
  description: "SAGES + Endoscapes with mean pooling and MLP head"
  hypothesis: "More training data should improve generalization"
