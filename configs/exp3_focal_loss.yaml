# Experiment 3: Attention Pooling + Focal Loss
# =============================================
# Purpose: Test if focal loss helps with CVS class imbalance
# Target: RTX 3080 12GB (Windows)
#
# Changes from exp2:
#   - Loss: Focal loss instead of BCE
#   - focal_alpha: 0.25 (or per-class)
#   - focal_gamma: 2.0
#
# Hypothesis: Focal loss will improve performance on rare positive cases
# by down-weighting easy negatives and focusing on hard examples.

# Paths (Windows local)
data:
  endoscapes_root: "C:/Users/sufia/Documents/Uni/Masters/DISSERTATION/endoscapes"
  sages_root: "C:/Users/sufia/Documents/Uni/Masters/DISSERTATION/sages_cvs_challenge_2025_r1"
  metadata_csv: "all_metadata.csv"
  train_vids: "train_vids.txt"
  val_vids: "val_vids.txt"
  test_vids: "test_vids.txt"
  results_dir: "C:/Users/sufia/Documents/Uni/Masters/DISSERTATION/vjepa/results/exp3_focal_loss"
  dataset_type: "endoscapes"

# Model - Same as exp2 (frozen backbone + attention pooling)
model:
  name: "facebook/vjepa2-vitl-fpc16-256-ssv2"
  freeze_backbone: true
  unfreeze_last_n_layers: 0
  hidden_dim: 1024
  classifier_hidden: 512
  num_classes: 3
  dropout: 0.5
  pooling_type: "attention"
  head_type: "mlp"
  attention_heads: 8
  attention_dropout: 0.1

# Data loading
dataset:
  num_frames: 16
  frame_step: 25
  resolution: 256
  augment_train: true
  horizontal_flip_prob: 0.5
  sages_val_ratio: 0.2

# Training - Same as exp2
training:
  batch_size: 32
  gradient_accumulation_steps: 1
  num_workers: 4
  mixed_precision: true
  epochs: 15
  optimizer: "adamw"
  learning_rate: 5.0e-4
  weight_decay: 0.1
  scheduler: "cosine"
  warmup_epochs: 2
  min_lr: 1.0e-6
  early_stopping: true
  patience: 5
  grad_clip: 1.0

# Loss - FOCAL LOSS (key change!)
loss:
  type: "focal"                   # Options: "bce_with_logits", "focal", "focal_with_pos_weight", "asymmetric"

  # Focal loss parameters
  focal_alpha: 0.25               # Balance factor (0.25 = more weight on negatives)
                                  # Can also be per-class: [0.3, 0.4, 0.5]
  focal_gamma: 2.0                # Focusing parameter
                                  # gamma=0: equivalent to BCE
                                  # gamma=2: standard focal loss (original paper)
                                  # gamma=5: very aggressive focusing on hard examples

  # Optional: combine with pos_weight (use type: "focal_with_pos_weight")
  # pos_weight: [2.0, 3.0, 4.0]   # Additional weight for positive samples

# Evaluation
evaluation:
  metrics: ["mAP", "AP_per_class", "balanced_accuracy", "f1"]
  threshold: 0.5

# Logging
logging:
  log_every_n_steps: 10
  save_every_n_epochs: 1

# Reproducibility
seed: 42

# Experiment metadata
experiment:
  name: "exp3_focal_loss"
  description: "Frozen V-JEPA + attention pooling + focal loss"
  hypothesis: "Focal loss improves CVS classification by focusing on hard examples"
  baseline: "exp2_local_attention"
  changes:
    - "Loss: BCE -> Focal Loss"
    - "focal_alpha: 0.25"
    - "focal_gamma: 2.0"
